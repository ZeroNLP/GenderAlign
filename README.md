# GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models
The data are described in the paper: "[GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2406.13925)". If you find the data useful, please cite the paper. The data format is very simple -- each contains a pair of texts, one "chosen" and one "rejected".

Disclaimer: The data contain content that may be offensive or upsetting. Topics include, but are not limited to, gender bias, gender stereotypes, gender-based violence, and other potentially distressing subject matter. Please engage with the data in accordance with your personal risk tolerance. The data are intended for research purposes, especially research aimed at reducing bias in models. The views expressed in the data do not reflect the views of the authors.
